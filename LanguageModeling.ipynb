{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vYyarhWgR9kJXCR4gM9oix1M4HZwo9Kn",
      "authorship_tag": "ABX9TyM7ON6VNdfeYimrV4iddKW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsogden/Bigram/blob/main/LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "haMCDG7lwgvD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/names.txt'\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "  names = f.read().splitlines()\n",
        "print(names[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOGK_u9Fw2Ei",
        "outputId": "5caee8a6-4d28-4aea-c8df-09625f5a20a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, names: list[str]):\n",
        "        self.chars = sorted(list(set(''.join(names))))\n",
        "        self.stoi = {s: i + 1 for i, s in enumerate(self.chars)}\n",
        "        self.stoi['.'] = 0\n",
        "        self.itos = {i: s for s, i in self.stoi.items()}\n",
        "        self.vocab_size = len(self.stoi)\n",
        "\n",
        "    def encode(self, name: str, max_length: int) -> list[int]:\n",
        "        padding = max_length - len(name)\n",
        "        return [self.stoi[c] for c in name] + [0] * (padding + 1)\n",
        "\n",
        "    def decode(self, tokens: list[int]) -> str:\n",
        "        return ''.join([self.itos[t] for t in tokens])"
      ],
      "metadata": {
        "id": "b53Un_-Iw-X5"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(names)\n",
        "max_length = max([len(name) for name in names])\n",
        "# print(max_length)\n",
        "encoded = torch.tensor([tokenizer.stoi[c] for name in names for c in name])\n",
        "print(encoded[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOlb6pAsxNLL",
        "outputId": "d2c7f9c2-7004-410e-effc-7d357013633c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4, 12, 12,  0, 14, 11,  8, 21,  8,  0,  0, 21,  0,  8, 18,  0,  1,  4,\n",
            "        11, 11,  0, 18, 14, 15,  7,  8,  0,  2,  7,  0, 17, 11, 14, 19, 19,  4,\n",
            "        12,  8,  0,  0, 12,  4, 11,  8,  0,  7,  0, 17, 15,  4, 17,  4, 21,  4,\n",
            "        11, 24, 13,  0,  1,  8,  6,  0,  8, 11,  4, 12,  8, 11, 24,  4, 11,  8,\n",
            "        25,  0,  1,  4, 19,  7, 12,  8, 11,  0,  4, 11, 11,  0,  0, 21,  4, 17,\n",
            "        24, 18, 14,  5,  8,  0,  2,  0, 12,  8, 11,  0,  0, 17,  8,  0, 18,  2,\n",
            "         0, 17, 11,  4, 19, 19, 21,  8,  2, 19, 14, 17,  8,  0, 12,  0,  3,  8,\n",
            "        18, 14, 13, 11, 20, 13,  0,  6, 17,  0,  2,  4,  2,  7, 11, 14,  4, 15,\n",
            "         4, 13,  4, 11, 14, 15,  4, 11,  0, 24, 11,  0, 17,  8, 11,  4, 24, 25,\n",
            "        14,  4, 24, 13, 14, 17,  0, 11,  8, 11, 24,  4, 11,  4,  0, 13, 14, 17,\n",
            "         7,  0, 13, 13,  0,  7, 11,  8, 11, 11,  8,  0, 13,  0,  3,  3,  8, 18,\n",
            "        14, 13,  0, 20,  1, 17,  4, 24,  4, 11, 11,  8,  4, 18, 19,  4, 11, 11,\n",
            "         0, 13,  0, 19,  0, 11,  8,  4, 25, 14,  4, 11,  4,  0,  7,  7,  0, 25,\n",
            "         4, 11, 21,  8, 14, 11,  4, 19,  0, 20, 17, 14, 17,  0, 18,  0, 21,  0,\n",
            "        13, 13,  0,  7,  0, 20,  3, 17,  4, 24,  1, 17, 14, 14, 10, 11, 24, 13,\n",
            "         1,  4, 11, 11,  0,  2, 11,  0,  8, 17,  4, 18, 10, 24, 11,  0, 17, 11,\n",
            "        20,  2, 24, 15,  0,  8, 18, 11,  4, 24,  4, 21,  4, 17, 11, 24,  0, 13,\n",
            "        13,  0,  2,  0, 17, 14, 11,  8, 13,  4, 13, 14, 21,  0,  6,  4, 13,  4,\n",
            "        18,  8, 18,  4, 12,  8, 11,  8,  0, 10,  4, 13, 13,  4,  3, 24, 18,  0,\n",
            "        12,  0, 13, 19,  7,  0, 12,  0, 24,  0, 22,  8, 11, 11, 14, 22, 10,  8,\n",
            "        13, 18, 11,  4, 24, 13,  0, 14, 12,  8,  0,  0, 11,  8, 24,  0,  7,  4,\n",
            "        11,  4, 13,  0, 18,  0, 17,  0,  7,  0, 17,  8,  0, 13,  0,  0, 11, 11,\n",
            "         8, 18, 14, 13,  6,  0,  1, 17,  8,  4, 11, 11,  0,  0, 11,  8,  2,  4,\n",
            "        12,  0,  3,  4, 11, 24, 13,  2, 14, 17,  0, 17, 20,  1, 24,  4, 21,  0,\n",
            "        18,  4, 17,  4, 13,  8, 19, 24,  0, 20, 19, 20, 12, 13,  0,  3,  4, 11,\n",
            "         8, 13,  4,  7,  0,  8, 11,  4, 24,  6,  8,  0, 13, 13,  0, 21,  0, 11,\n",
            "         4, 13, 19,  8, 13,  0,  8, 18, 11,  0,  4, 11,  8,  0, 13,  0, 16, 20,\n",
            "         8, 13, 13, 13,  4, 21,  0,  4,  7,  8, 21, 24, 18,  0,  3,  8,  4, 15,\n",
            "         8, 15,  4, 17, 11, 24,  3,  8,  0,  0, 11,  4, 23,  0,  9, 14, 18,  4,\n",
            "        15,  7,  8, 13,  4,  4, 12,  4, 17, 24,  9, 20, 11,  8,  0,  3,  4, 11,\n",
            "         8, 11,  0,  7,  0, 17,  8,  0, 13, 13,  0, 21,  8, 21,  8,  0, 13, 10,\n",
            "         0, 24, 11,  4,  4, 18, 14, 15,  7,  8,  4,  1, 17,  8,  4, 11, 11,  4,\n",
            "        12,  0,  3,  4, 11,  8, 13,  4, 15,  4, 24, 19, 14, 13, 17, 24, 11,  4,\n",
            "         4,  2, 11,  0, 17,  0,  7,  0,  3, 11,  4, 24, 12,  4, 11,  0, 13,  8,\n",
            "         4, 12,  0,  2, 10,  4, 13, 25,  8,  4, 17,  4,  0,  6,  0, 13,  0,  3,\n",
            "         0, 11, 24, 13, 13, 11,  8, 11,  8,  0, 13,  0,  0, 20,  1, 17,  4,  4,\n",
            "         9,  0,  3,  4, 10,  0, 19,  7,  4, 17,  8, 13,  4,  8, 18,  0,  1,  4,\n",
            "        11, 11,  4, 13,  0, 19,  0, 11,  8,  0, 17,  0,  4, 11, 24, 13, 13, 12,\n",
            "         0, 17,  8,  0,  0, 19,  7,  4, 13,  0, 23,  8, 12,  4, 13,  0,  0, 17,\n",
            "        24,  0, 11,  4,  8, 11,  0, 13,  8, 19,  0, 24, 11, 14, 17,  5,  0,  8,\n",
            "        19,  7, 17, 14, 18,  4, 10, 24, 11,  8,  4,  0, 11,  4, 23,  0, 13,  3,\n",
            "        17,  0, 12,  0, 17, 24, 12,  0, 17,  6,  0, 17,  4, 19, 11, 24, 11,  0,\n",
            "         0, 18,  7, 11,  4, 24,  0, 12,  0, 24,  0,  4, 11,  8, 25,  0,  1, 17,\n",
            "         8,  0, 13, 13,  0,  1,  0,  8, 11,  4, 24,  0, 13,  3, 17,  4,  0, 10,\n",
            "         7, 11, 14,  4,  9,  0, 18, 12,  8, 13,  4, 12,  4, 11, 14,  3, 24,  8,\n",
            "        17,  8, 18,  8, 18,  0,  1,  4, 11, 13, 14, 17,  0,  7,  0, 13, 13,  0,\n",
            "         1,  4, 11, 11,  4, 21,  0, 11,  4, 17,  8,  0,  4, 12,  4, 17, 18, 14,\n",
            "        13,  0,  3,  0, 11, 24, 13, 17, 24, 11,  4,  8,  6,  7,  4,  3,  4, 13,\n",
            "         4, 12,  4, 17, 18, 24, 13,  0, 13,  0, 18, 19,  0, 18,  8,  0, 10,  0,\n",
            "        24, 11,  0,  0, 11, 24, 18, 18,  0,  9, 20, 11,  8,  0, 13,  0,  2,  7,\n",
            "         0, 17, 11,  8,  4,  4, 18, 19,  7,  4, 17,  0, 17,  8,  4, 11,  2,  4,\n",
            "         2,  8, 11,  8,  0, 21,  0, 11,  4, 17,  8,  4,  0, 11,  8, 13,  0, 12,\n",
            "        14, 11, 11, 24, 17,  4,  4, 18,  4,  0, 11,  8, 24,  0,  7, 11,  8, 11,\n",
            "        11, 24, 15,  0, 17, 10,  4, 17,  5,  8, 13, 11,  4, 24, 12, 14, 17,  6,\n",
            "         0, 13, 18, 24,  3, 13,  4, 24,  9, 14, 17,  3, 24, 13,  4, 11, 14,  8,\n",
            "        18,  4, 19, 17,  8, 13,  8, 19, 24,  3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(encoded))\n",
        "train_data = encoded[:n]\n",
        "val_data = encoded[n:]\n",
        "\n",
        "block_size = 8\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t + 1]\n",
        "    target = y[t]\n",
        "    print(f'{context} --> {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXxguYxB7eHE",
        "outputId": "a2b1c3af-953b-4863-c0c1-b9740ce3c08e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4]) --> 12\n",
            "tensor([ 4, 12]) --> 12\n",
            "tensor([ 4, 12, 12]) --> 0\n",
            "tensor([ 4, 12, 12,  0]) --> 14\n",
            "tensor([ 4, 12, 12,  0, 14]) --> 11\n",
            "tensor([ 4, 12, 12,  0, 14, 11]) --> 8\n",
            "tensor([ 4, 12, 12,  0, 14, 11,  8]) --> 21\n",
            "tensor([ 4, 12, 12,  0, 14, 11,  8, 21]) --> 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x, y\n",
        "x, y = get_batch('train')"
      ],
      "metadata": {
        "id": "cmC4arEN71Fq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx) # B, T, C\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T , C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B, 1\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(tokenizer.vocab_size)\n",
        "logits, loss = m(x, y)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "decode = tokenizer.decode(m.generate(idx, max_new_tokens=15)[0].tolist())\n",
        "print(decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na7osdY_5GXI",
        "outputId": "db4c0b70-2411-4516-fc3c-2b0e98c0c81d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 26])\n",
            "tensor(3.8399, grad_fn=<NllLossBackward0>)\n",
            "awqvfnabpgcecctt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeCZ3eoG9fgR",
        "outputId": "ea357275-212d-4b4e-aadb-f264c4517795"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.593684673309326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "decode = tokenizer.decode(m.generate(idx, max_new_tokens=15)[0].tolist())\n",
        "print(decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6bM2_2O_dVT",
        "outputId": "17dc6375-24fc-46be-8770-6a3ab8acf3bf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ajalyvytynkecela\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = torch.tensor([tokenizer.encode(name, 15) for name in names])\n",
        "print(encoded[:1000].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbS24yQQ_l3f",
        "outputId": "1224b0b4-1eca-4fcc-a1ae-6a280235c1d7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        if targets is None:\n",
        "            logits = self.token_embedding_table(inputs)\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = self.token_embedding_table(inputs)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx"
      ],
      "metadata": {
        "id": "FkSCa7RKBo8J"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = SimpleModel(tokenizer.vocab_size)\n",
        "logits, loss = m(encoded, encoded)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJSe85qACgXu",
        "outputId": "e2993103-3f96-4d1d-8578-43cc40b036b3"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512528, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "decode = tokenizer.decode(m.generate(idx, max_new_tokens=15)[0].tolist())\n",
        "print(decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpZuG5OTDJ-k",
        "outputId": "fdfd623d-bd72-4204-afb5-64e0387c1a00"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aocxxmdliewdzfqj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "m = SimpleModel(tokenizer.vocab_size)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
        "for epoch in range(epochs):\n",
        "    m.train()\n",
        "    running_loss = 0.0\n",
        "    for i in range(0, len(encoded) - batch_size, batch_size):\n",
        "        inputs = encoded[i: i + batch_size]\n",
        "        targets = encoded[i + 1: i + batch_size + 1]\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = m(inputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    running_loss /= len(encoded) / batch_size\n",
        "    if epoch % 10 == 0 or (epoch == epochs - 1):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss:0.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOtSUHixDZGC",
        "outputId": "501577df-e772-4daa-b1c8-b9c66323787b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 4.1618\n",
            "Epoch 11/100, Loss: 1.1135\n",
            "Epoch 21/100, Loss: 1.1107\n",
            "Epoch 31/100, Loss: 1.1106\n",
            "Epoch 41/100, Loss: 1.1105\n",
            "Epoch 51/100, Loss: 1.1105\n",
            "Epoch 61/100, Loss: 1.1105\n",
            "Epoch 71/100, Loss: 1.1105\n",
            "Epoch 81/100, Loss: 1.1105\n",
            "Epoch 91/100, Loss: 1.1105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "decode = tokenizer.decode(m.generate(idx, max_new_tokens=16)[0].tolist())\n",
        "print(decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q8eP4KmDqxS",
        "outputId": "50381109-8b38-4f97-ef09-868083b516d1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aaaasssaaaaaaaaaa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        if targets is None:\n",
        "            logits = self.token_embedding_table(inputs)\n",
        "            loss = None\n",
        "        else:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            logits = self.linear(embed)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx"
      ],
      "metadata": {
        "id": "HY_3Svy0GMjj"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "m = SimpleModel(tokenizer.vocab_size)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
        "for epoch in range(epochs):\n",
        "    m.train()\n",
        "    running_loss = 0.0\n",
        "    for i in range(0, len(encoded) - batch_size, batch_size):\n",
        "        inputs = encoded[i: i + batch_size]\n",
        "        targets = encoded[i + 1: i + batch_size + 1]\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = m(inputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    running_loss /= len(encoded) / batch_size\n",
        "    if epoch % 10 == 0 or (epoch == epochs - 1):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss:0.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUIHzFaZGzmG",
        "outputId": "5a446fad-bfa7-4c8b-89c2-157e9eabb9d2"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 1.2847\n",
            "Epoch 11/100, Loss: 1.1072\n",
            "Epoch 21/100, Loss: 1.1068\n",
            "Epoch 31/100, Loss: 1.1069\n",
            "Epoch 41/100, Loss: 1.1069\n",
            "Epoch 51/100, Loss: 1.1071\n",
            "Epoch 61/100, Loss: 1.1072\n",
            "Epoch 71/100, Loss: 1.1073\n",
            "Epoch 81/100, Loss: 1.1074\n",
            "Epoch 91/100, Loss: 1.1075\n",
            "Epoch 100/100, Loss: 1.1076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "decode = tokenizer.decode(m.generate(idx, max_new_tokens=5)[0].tolist())\n",
        "print(decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1P8IfX0G1GU",
        "outputId": "39f1284b-2b43-4a07-9bfa-87a07af8acdb"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anwfnh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "  names_new = f.read()\n",
        "print(names_new[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkAA3GMAJB9J",
        "outputId": "e68f9349-af2a-415c-f72f-916c8a6c78df"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emma\n",
            "olivi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(names_new)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(''.join(chars))\n",
        "stoi = {s: i for i, s in enumerate(chars)}\n",
        "itos = {i: s for s, i in stoi.items()}\n",
        "print(stoi)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dotpMfVJbF4",
        "outputId": "3cb3ab34-d654-4348-cceb-c43a737024c3"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "\n",
            "abcdefghijklmnopqrstuvwxyz\n",
            "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor([stoi[c] for c in names_new])\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x, y\n",
        "x, y = get_batch('train')\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj7SEk5AJ9Py",
        "outputId": "44c06350-7d6d-487f-c940-2452ebed791f"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8]) torch.Size([4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        if targets is None:\n",
        "            logits = self.token_embedding_table(inputs)\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = self.token_embedding_table(inputs)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Xfn8mhAaL-nC"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = SimpleModel(vocab_size)\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv5gosOVNZLi",
        "outputId": "b3db6222-eec5-4fb6-a4f4-5e5ca93c1dd3"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4008350372314453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((100, 1), dtype=torch.long)\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "decoded = decode([l for l in m.generate(idx, max_new_tokens=26)[0].tolist()])\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU2Qsv2wN-yG",
        "outputId": "b24f628b-d058-4ff9-adcf-3a9d73b7d9fd"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "fus\n",
            "aiecuqcon\n",
            "yodoe\n",
            "listei\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((5, 1), dtype=torch.long)\n",
        "# decoded = decode([l for l in m.generate(idx, max_new_tokens=26).tolist()])\n",
        "vals = [decode(l).split('\\n') for l in m.generate(idx, max_new_tokens=26).tolist()]\n",
        "vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ_FQdm-Pa-N",
        "outputId": "22d13078-39fa-4fee-f3ac-364b62a590c4"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['', 'say', 'jerimyan', 'tabeaki', 'ane', 'a'],\n",
              " ['', 'me', 't', 'vel', 'ncanteson', 'ikillky'],\n",
              " ['', 'kyasein', 'ka', 'wa', 'corahorvaroh'],\n",
              " ['', 'anale', 'e', 'letstellesttailari'],\n",
              " ['', 'a', 'kencri', 'anavellyatu', 'ny', 'ly']]"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.rnn = nn.LSTM(\n",
        "            vocab_size, vocab_size, batch_first=True\n",
        "        )\n",
        "        self.logits = nn.Linear(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        if targets is None:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            rnn_output, _ = self.rnn(embed)\n",
        "            logits = self.logits(rnn_output)\n",
        "            loss = None\n",
        "        else:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            rnn_output, _ = self.rnn(embed)\n",
        "            logits = self.logits(rnn_output)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx"
      ],
      "metadata": {
        "id": "47Z2qU7pRVaF"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = SimpleModel(vocab_size)\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8tahpwYT3jc",
        "outputId": "cbd66044-b43b-4adf-84f9-2f3a4d839419"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.136561155319214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((10, 1), dtype=torch.long)\n",
        "vals = [decode(l).split('\\n') for l in m.generate(idx, max_new_tokens=26).tolist()]\n",
        "for val in vals:\n",
        "  print(val[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO-XPUMHT9D8",
        "outputId": "813f4e57-6131-44b3-c990-3721ea6915c8"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ovel', 'lucka', 'qeita', 'mirkah', 've']\n",
            "['tilloiza', 'chabreigh', 'pacleil']\n",
            "['averion', 'mikhya', 'zayen', 'dakel']\n",
            "['adlosel', 'caleelyn', 'zairanie', '']\n",
            "['saiven', 'maysa', 'colstra', 'alion']\n",
            "['losya', 'jayona', 'sakona', 'tarist']\n",
            "['nehiphariah', 'nistin', 'joni', 'jh']\n",
            "['adalynn', 'mecaira', 'hami', 'abrde']\n",
            "['khath', 'sakios', 'lonakeeyn', 'lec']\n",
            "['benden', 'sasevian', 'terren', 'kan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.V = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, query, keys, d_model=512):\n",
        "        Q = self.Q(query)\n",
        "        K = self.K(keys)\n",
        "        V = self.V(keys)\n",
        "        attn_weights = torch.bmm(Q, K.transpose(1, 2))\n",
        "        attn_weights = F.softmax(attn_weights / d_model ** 0.5, dim=-1)\n",
        "        attn_output = torch.bmm(attn_weights, V)\n",
        "        return attn_output\n",
        "\n",
        "attention = Attention(10)\n",
        "embedding = nn.Embedding(10, 10)\n",
        "\n",
        "query = embedding(torch.randint(low=0, high=9, size=(10, 10)))\n",
        "keys = embedding(torch.randint(low=0, high=9, size=(10, 10)))\n",
        "attn_output = attention(query, keys)\n",
        "print(attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0w7taGQUGne",
        "outputId": "0ad12b04-4d64-49ee-d3d5-f3a4d2239209"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(attn_output + query).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cz-lHyIbY98",
        "outputId": "6305bf93-dee5-4075-adcd-fb62dd1b1f93"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IntermediateModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.attention = Attention(vocab_size)\n",
        "        self.logits = nn.Linear(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        if targets is None:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            attn_output = self.attention(embed, embed, 27)\n",
        "            logits = self.logits(embed + attn_output)\n",
        "            loss = None\n",
        "        else:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            attn_output = self.attention(embed, embed, 27)\n",
        "            logits = self.logits(embed + attn_output)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx"
      ],
      "metadata": {
        "id": "vEwdioEZbpZL"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = IntermediateModel(vocab_size)\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PyIh2qDcczA",
        "outputId": "e7c38f86-2e69-4acd-b669-29f9551e94c5"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.398796558380127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((10, 1), dtype=torch.long)\n",
        "vals = [decode(l).split('\\n') for l in m.generate(idx, max_new_tokens=26).tolist()]\n",
        "for val in vals:\n",
        "  print(val[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCe_vRrjcpo9",
        "outputId": "80840f42-a11e-4fbe-b41b-49df5b0bdb18"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n', 'a', 'ssacanyle', 'e', 'li', 'sshaman']\n",
            "['aiahaharasaiyadashaiayarah']\n",
            "['e', 'zaia', 'lalirissivilllirena']\n",
            "['se', 'dh', 'min', 's', 'lini', 'manaquura']\n",
            "['tronlonnnonne', 'evpessronceg']\n",
            "['ayziziaiara', 'h', 'zaa', 'aishah', 'n']\n",
            "['yahishaeyah', 'saraa', 'h', 'aneili']\n",
            "['d', 'tzoiswai', 'bek', 'hmitrneeya', '']\n",
            "['ahta', 'lalyllis', 'calavobiri', 'a']\n",
            "['a', 'aryalay', 'an', 'aliarahie', 'a', 'i']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IntermediateModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.attention = Attention(vocab_size)\n",
        "        self.proj = nn.Linear(vocab_size, vocab_size)\n",
        "        self.norm = nn.LayerNorm(vocab_size)\n",
        "        self.logits = nn.Linear(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets=None):\n",
        "        if targets is None:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            attn_output = self.attention(embed, embed, 27)\n",
        "            normed = self.norm(attn_output + embed)\n",
        "            proj = self.proj(normed)\n",
        "            normed = self.norm(normed + proj)\n",
        "            logits = self.logits(normed)\n",
        "            loss = None\n",
        "        else:\n",
        "            embed = self.token_embedding_table(inputs)\n",
        "            attn_output = self.attention(embed, embed, 27)\n",
        "            normed = self.norm(attn_output + embed)\n",
        "            proj = self.proj(normed)\n",
        "            normed = self.norm(normed + proj)\n",
        "            logits = self.logits(normed)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # B, C\n",
        "            probs = F.softmax(logits, dim=-1) # generate probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # B, T + 1\n",
        "        return idx"
      ],
      "metadata": {
        "id": "5YKbDlsQc2xL"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = IntermediateModel(vocab_size)\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSFllch9dl5r",
        "outputId": "329d61ee-01fc-4e8f-da64-29cfa623c067"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.460574150085449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((10, 1), dtype=torch.long)\n",
        "vals = [decode(l).split('\\n') for l in m.generate(idx, max_new_tokens=26).tolist()]\n",
        "for val in vals:\n",
        "  print(val[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X99-3d54dnjp",
        "outputId": "b3037ac8-0c8c-497b-dca0-9c124775a41c"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wthaya', 'telalllu', 'ma', 'tynazon']\n",
            "['er', 'rie', 'ry', 'yif', 'de', 'bricerida']\n",
            "['gaitha', 'dabi', 'alailrana', 'elol']\n",
            "['a', 'owaliessaeebemeyadan', 'sil']\n",
            "['nahlch', 'sstonn', 'xyanerolyn', 'h']\n",
            "['alilynivi', 'je', 'jadav', 'ja', 'jh', 'f']\n",
            "['keelighanyannjaveinaoky', 'rm']\n",
            "['a', 'gahan', 'ka', 'kahn', 'n', 'ste', 'synn']\n",
            "['riliah', 'kan', 'zach', 'kedlayhliz']\n",
            "['aie', 'nnysa', 'aryaru', 'tailormch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = torch.linspace(0, 26, 27)\n",
        "for i in range(pos.shape[0] // 2):\n",
        "    pos[2 * i] = torch.sin(pos[2 * i] / 10000 ** (2 * i / 27))\n",
        "    pos[2 * i + 1] = torch.cos(pos[2 * i + 1] / 10000 ** (2 * i / 27))\n",
        "pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c8AL7VCfgJW",
        "outputId": "3cb6fba6-9e8a-4543-d5df-9d693215e304"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000e+00, 5.4030e-01, 8.4734e-01, 5.4331e-02, 8.5317e-01, 2.8906e-01,\n",
              "        6.9967e-01, 6.1841e-01, 4.9886e-01, 8.3229e-01, 3.2405e-01, 9.3483e-01,\n",
              "        1.9884e-01, 9.7658e-01, 1.1777e-01, 9.9201e-01, 6.8142e-02, 9.9738e-01,\n",
              "        3.8770e-02, 9.9916e-01, 2.1779e-02, 9.9974e-01, 1.2110e-02, 9.9992e-01,\n",
              "        6.6781e-03, 9.9998e-01, 2.6000e+01])"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYZtQJO1gEjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}