{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16F9VW8qCqgIqu_1dYFgoeHe0EWVQgVNA",
      "authorship_tag": "ABX9TyMYRYbUfNNxcKjzzzPhLL9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsogden/Bigram/blob/main/Bigram_Toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KWsGDEBEm4zz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/names.txt'\n",
        "words = []\n",
        "with open(file_path, 'r') as f:\n",
        "    for word in f.readlines():\n",
        "        words.append(word.replace('\\n', ''))"
      ],
      "metadata": {
        "id": "_YE-ZWItnXe3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "for word in words:\n",
        "    for char in word:\n",
        "        vocab.add(char)\n",
        "vocab = sorted(list(vocab))\n",
        "print(vocab, len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMJ3DSKWn_I-",
        "outputId": "517a45a5-06eb-4456-beed-c7f159a9b375"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = {char: idx + 1 for idx, char in enumerate(vocab)}\n",
        "encoder['.'] = 0\n",
        "decoder = {idx: char for idx, char in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "kKWKx7qAt5Uv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "offset = 3\n",
        "sample = words\n",
        "for word in sample:\n",
        "    context = [0] * offset\n",
        "    for char in word + '.':\n",
        "        idx = encoder[char]\n",
        "        X.append(context)\n",
        "        y.append(idx)\n",
        "        # print(context, encoder[char])\n",
        "        context = context[1:] + [idx]\n",
        "X, y = torch.tensor(X), torch.tensor(y)"
      ],
      "metadata": {
        "id": "v3IzU6-4oR4j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHmj_KGspcwu",
        "outputId": "a40a793f-5eb5-4a00-90dd-7daf433e12b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([228146, 3]), torch.Size([228146]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[:10], y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwfIrY1_u-yY",
        "outputId": "e1e9f276-42be-44ad-ce7e-4e9b46c34de4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  0,  0],\n",
              "         [ 0,  0,  5],\n",
              "         [ 0,  5, 13],\n",
              "         [ 5, 13, 13],\n",
              "         [13, 13,  1],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0, 15],\n",
              "         [ 0, 15, 12],\n",
              "         [15, 12,  9],\n",
              "         [12,  9, 22]]),\n",
              " tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Bigram(nn.Module):\n",
        "    def __init__(\n",
        "            self, block_size, num_embeddings, embedding_dim, hidden_dim, output_dim\n",
        "        ):\n",
        "        super(Bigram, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.hidden = nn.Linear(embedding_dim * block_size, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embed = self.embedding(inputs)\n",
        "        proj = self.hidden(torch.cat(torch.unbind(embed, 1), 1))\n",
        "        output = self.output(F.tanh(proj))\n",
        "        return output"
      ],
      "metadata": {
        "id": "_nfl4L1pxgVW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device = 'cuda:0'\n",
        "torch.cuda.device(device)\n",
        "\n",
        "X_train = X[:int(0.1 * X.shape[0])]\n",
        "y_train = y[:int(0.1 * y.shape[0])]\n",
        "\n",
        "block_size = offset\n",
        "num_embeddings = len(vocab) + 1\n",
        "embedding_dim = 5\n",
        "hidden_dim = 100\n",
        "output_dim = len(vocab) + 1\n",
        "model = Bigram(\n",
        "    block_size, num_embeddings, embedding_dim, hidden_dim, output_dim\n",
        ").to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=3e-4)\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    running_loss = 0\n",
        "    for batch in range(X.shape[0] - batch_size):\n",
        "        x_ = X[batch: batch + batch_size]\n",
        "        y_ = y[batch: batch + batch_size]\n",
        "        preds = model(x_.to(device))\n",
        "        loss = loss_fn(preds, y_.to(device))\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    running_loss /= batch + 1\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}, loss = {running_loss:0.4f}')"
      ],
      "metadata": {
        "id": "UQHL8EtByZ6T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvbQl5MHz0YD"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}